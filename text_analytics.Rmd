---
title: "twitter_text_analysis"
author: "Kristin Robinson"
date: "April 26, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(lubridate)
library(ggplot2)
library(dplyr)
library(readr)
```

```{r}
# load data - longitude and latitude already transformed
twitter_data <- read.csv("data/twitter_31days.csv")
twitter_data$Timestamp<-as.POSIXct(twitter_data$Timestamp)

# set eruption date and time
day1 <- as.Date("2014-08-24")
hour1 <- as.POSIXct("2014-08-24 04:20:44")

# add columns for elapsed days
twitter_data$days <- difftime (as.Date(twitter_data$Timestamp), day1, units = "days")
twitter_data$days2 <- difftime(twitter_data$Timestamp, hour1, units = "days")

# add columns for elapsed minutes
twitter_data$minutes <- difftime(twitter_data$Timestamp, hour1, units = "mins")
twitter_data$minutes2 <-trunc(twitter_data$minutes, "mins")
```

```

```{r}
# clean up text and break down tweets by word

library(tidytext)
library(stringr)

replace_reg <- "https://t.co/[A-Za-z\\d]+|http://[A-Za-z\\d]+|&amp;|&lt;|&gt;|RT|https"
unnest_reg <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"

tidy_tweets <- twitter_data %>%
  mutate(Text = str_replace_all(Text, replace_reg, "")) %>%
  unnest_tokens(word, Text, token = "regex", pattern = unnest_reg) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]"))


```

```{r}

# frequency count of words
word_count <- tidy_tweets %>% 
  count(word, sort = TRUE)
total <- sum(word_count$n)
word_count$frequency <- word_count$n/total

```

```{r}

# plot number of tweets by distance from epicenter
ggplot(twitter_data, aes(dist)) +
  geom_freqpoly(binwidth = 1) +
  labs(x = "miles from epicenter", title = "# of Tweets by Distance from Epicenter")

# plot number of tweets by time by distance (distance <= 500 miles)
ggplot(twitter_data, aes(x = Timestamp, y = dist)) + geom_point() + labs(title="Tweets by distance by time\n(distance < 500 miles)", x = "time", y="distance (miles)")

```



